# MPhil-Research-Project
My research project for MPhil degree
üö¢ Ship Detection in the Exclusive Economic Zone of Mauritius in Sentinel-2 Imagery using CNNs & Yolov11

üôè Acknowledgements


This project wouldn‚Äôt have been possible without the incredible people who supported me along the way.
First, I‚Äôm grateful to my supervisors ‚Äî Assoc. Prof. Heeralall-Issur N., Prof. Cullen D., and Dr. Beeharry Y. ‚Äî for their steady guidance, thoughtful feedback, and patience throughout the research journey. To my family ‚Äî my mother Anita, my brothers Ranveer, Yashwant, and Vrisht, and my sister-in-law Youri ‚Äî thank you for believing in me and cheering me on through every challenge. My friends Heerveen, Dinesh, and Nawshine also deserve special mention for their encouragement and for keeping me motivated when things got tough. A big thanks as well to Ruben Louis for his timely technical support. I‚Äôm also thankful to the DARA (Development in Africa with Radio Astronomy) scholarship, which funded and enabled this research, and to the examiners whose detailed feedback pushed me to refine my approach and reach better results. A special shoutout goes to new supporters I met along the way, like Sofia, whose encouragement during a challenging transition helped me stay the course. And finally ‚Äî yes, I‚Äôll admit it ‚Äî even ChatGPT deserves a thank-you. Its help with debugging, research, and drafting made the process faster (and less painful).


üìå Overview


Maritime surveillance is important for national security, economic resilience, and environmental protection. For island nations like Mauritius, whose Exclusive Economic Zone (EEZ) spans an impressive 2.3 million km¬≤, the third largest in Africa and the 25th largest worldwide, safeguarding maritime borders is not only a matter of sovereignty but also of survival. The Mauritian EEZ lies across busy international shipping routes connecting Asia to Africa and beyond, with vessels frequently carrying hazardous cargo and large volumes of goods. This geo-strategic position presents unique challenges, including illegal fishing, piracy, narcotics trafficking, marine pollution, and accident prevention.
  

Traditional surveillance methods such as naval patrols and radar monitoring, while effective, have limited coverage. Satellite imagery offers a scalable and cost-effective alternative, providing continuous monitoring over vast ocean areas. Both optical imagery (visible spectrum) and Synthetic Aperture Radar (SAR) have proven valuable. Optical imagery captures fine details of ships during clear conditions, while SAR enables detection at night or through clouds. However, extracting meaningful insights from these large volumes of satellite data remains a challenge.


Conventional ship detection techniques often rely on handcrafted features and classical classifiers, which can be brittle under variations in ship size, orientation, or atmospheric conditions. Recent advances in deep learning, particularly Convolutional Neural Networks (CNNs) and real-time detectors such as YOLO, provide a promising alternative. These models excel at learning complex patterns directly from data, offering improved robustness and accuracy compared to traditional methods.


This study leverages Sentinel-2 optical satellite imagery, chosen for its free accessibility, 10-meter resolution, and five-day revisit frequency ‚Äî making it suitable for monitoring ships traversing the Mauritian EEZ, where a cargo vessel typically requires six days to cross its full extent. Using Sentinel-2 data ensures not only affordability but also the ability to build a custom dataset tailored to local conditions, including coral reefs, ‚Äúwhite water‚Äù from wave turbulence, and frequent cloud cover.


The research specifically evaluates and compares state-of-the-art CNN architectures (such as VGG-16, ResNet-50, Inception-V3, DenseNet, EfficientNet, and MobileNet) against the YOLOv11 framework. By applying transfer learning, the models were fine-tuned on a custom dataset, enabling effective training despite limited labeled data. Performance was measured in terms of precision, recall, F1-score, and inference speed, balancing accuracy against computational efficiency. Beyond technical performance, the study also explores broader implications:
- How well deep learning models generalize to other datasets and geographic regions.
- The trade-offs between high accuracy and real-time detection needs.
- The interpretability of model outputs for end-users like maritime authorities.
- The potential role of satellite-based ship detection systems in enhancing Maritime Domain Awareness (MDA) in small developing states.



To summarise, this research demonstrates that CNNs and YOLO offer significant potential for affordable, scalable, and effective maritime surveillance in Mauritius and similar contexts. While challenges remain in terms of dataset availability, computational costs, and real-world deployment, the results highlight a viable path toward integrating AI-driven ship detection into national defense and environmental monitoring strategies.



Available overview of optical satellite datasets utilised for ship detection tasks.

This collection highlights the diversity of datasets used in maritime object detection, ranging from high-resolution commercial data to publicly available data with varying revisit times and resolutions. 

<img width="545" height="720" alt="image" src="https://github.com/user-attachments/assets/2868b308-bcf1-46f2-a05f-824ea73b8bbc" />



üõ∞Ô∏è Sentinel-2 Satellite


The Sentinel-2 mission, operated by the European Space Agency (ESA), is a medium- to high-resolution Earth observation program designed for environmental monitoring, land use analysis, and maritime surveillance. It consists of twin satellites in the same orbit, phased 180¬∞ apart, ensuring a revisit time of five days at the Equator. Each satellite is equipped with the Multi-Spectral Instrument (MSI), capturing data in 13 spectral bands: four at 10 m, six at 20 m, and three at 60 m resolution, across a swath width of 290 km.


Sentinel-2 data are freely available and provide a cost-effective alternative to commercial imagery, which can be prohibitively expensive for small island states. For this research, Sentinel-2 was chosen as the optimal balance of spatial resolution, temporal frequency, and accessibility, making it well suited for monitoring large maritime areas such as Mauritius‚Äô Exclusive Economic Zone (EEZ).


The satellites collect data over land, coastal regions, and islands worldwide, including inland water bodies and closed seas. Data are provided in two main product levels:
- Level-1C: Top-Of-Atmosphere (TOA) reflectance, orthorectified for geometric accuracy. (~600 MB per 100√ó100 km¬≤ tile)
- Level-2A: Bottom-Of-Atmosphere (BOA) reflectance, atmospherically corrected for surface-level analysis. (~800 MB per tile)


Both products are distributed as standardised 100√ó100 km¬≤ ortho-image tiles in the Universal Transverse Mercator (UTM/WGS84) projection, ensuring spatial consistency and seamless integration with other geospatial datasets.


The UTM projection is critical for precise vessel detection, as it allows Sentinel-2 imagery to align with maritime boundaries, AIS vessel data, and port infrastructure. For Mauritius, whose vast EEZ spans multiple UTM zones, this ensures reliable monitoring of shipping lanes, vessel movements, and potential threats.


<img width="728" height="386" alt="image" src="https://github.com/user-attachments/assets/2bf1711b-a57b-497d-a917-cba24b96f325" />


üìö Literature Review: Advances in Ship Detection


Ship detection in satellite imagery has evolved from traditional image processing methods with handcrafted features (HOG, SIFT) to deep learning-based approaches that learn robust patterns directly from data.


üîπ Convolutional Neural Networks (CNNs)
- CNNs introduced automated feature extraction, making them far more effective than traditional methods in noisy maritime environments.
- Two-stage CNN frameworks (e.g., R-CNN family) achieve high precision but are computationally expensive.
- Single-stage CNN frameworks (e.g., SSD, MobileNet variants) are more efficient, enabling near real-time applications.
- Specialised innovations such as multi-scale feature fusion and attention mechanisms improved detection across diverse ship sizes and complex backgrounds.


üîπ YOLO (You Only Look Once)
- YOLO revolutionised real-time detection by combining classification and localisation in a single forward pass.
- Recent versions (YOLOv5‚ÄìYOLOv11) integrate transformer modules, spatial attention, and multi-scale detection, making them highly effective in maritime contexts.
- Studies applying YOLO to Sentinel-2 imagery have achieved strong precision/recall trade-offs, making it suitable for operational monitoring.


üîπ Foundation Models & Emerging Trends
- Foundation models (DETR, DINO, Florence-2) leverage large-scale pretraining to generalise across tasks with minimal labelled data.
- They capture global context, making them robust in noisy environments with clouds, waves, and occlusions.
- Limitations include high computational costs, domain adaptation challenges, and interpretability issues.


üîπ Key Insights
- CNNs excel in accuracy but are resource-heavy.
- YOLO balances speed and accuracy, ideal for real-time ship monitoring.
- Foundation models offer scalability and few-shot learning but require heavy compute.
- Research highlights a trade-off between precision and efficiency, shaping how models are chosen for maritime surveillance.


üìÇ Dataset Creation for CNN


Since no ready-to-use Sentinel-2 ship detection dataset was available, a custom dataset was built from scratch using Sentinel-2 optical imagery collected between March 2016 and March 2021.


<img width="700" height="479" alt="image" src="https://github.com/user-attachments/assets/81b797dd-6867-4580-939b-7d6daf57ef3c" />


Steps Taken
- Image Collection:
  - Downloaded 98 Sentinel-2 images in .SAFE format.
  - Each contained 13 bands; RGB images were generated manually by stacking B02 (Blue), B03 (Green), and B04 (Red) bands instead of using the prebuilt TCI, to ensure consistency.


- Ship Extraction:
  - Images were analysed in QGIS 3.10.
  - 522 ship instances were manually identified and cropped.


- Image Preprocessing:
  - Crops were resized to 49√ó49 pixels, chosen to capture the largest visible ships in Sentinel-2 data.
  - To improve classification accuracy, the dataset was expanded to five classes:
    - Ship (522)
    - Cloud (521)
    - Sea (123)
    - Land (159)
    - Coast (60)
  - Total: 1385 labelled instances.


<img width="758" height="608" alt="image" src="https://github.com/user-attachments/assets/169844a3-2bd7-4a0e-a38b-85676b01712f" />


- Annotation Challenges:
  -  Many annotation tools didn‚Äôt support large JPEG2000 files or raw band stacking.
  -  QGIS was chosen as the annotation tool since it could process the raw bands directly and generate raster layers without quality loss.

Why this approach?
- Unlike large public datasets (e.g., xView, DIOR), this dataset was tailored specifically to Sentinel-2 imagery and the Mauritian EEZ context.
- Instead of fine-grained ship classification (e.g., tankers, fishing vessels), the focus was on distinguishing ships from common maritime backgrounds like sea, land, clouds, and coastlines.


üìÇ Dataset Creation for YOLO


For the YOLO-based approach, a custom dataset was created from Sentinel-2 imagery (March 2019 ‚Äì March 2021) to enable real-time ship detection.

Steps Taken
- Image Collection & Preprocessing:
  - Used 98 Sentinel-2 raw tiles, each 10,000 √ó 10,000 pixels.
  - Split them into smaller 2,000 √ó 2,000 sub-tiles to maintain ship visibility while making images manageable for YOLO.
  - From 1,501 sub-tiles generated, 156 relevant sub-tiles were selected for annotation.

- Annotation:
  - Ships were annotated with bounding boxes using LabelImg.
  - Annotations followed YOLO‚Äôs required format:
      class center_x center_y width height (all coordinates normalised by image dimensions).

  - A single class (‚ÄúShip‚Äù) was used to avoid class imbalance and focus the model on vessel detection.

- Dataset Composition:
  - 504 total ship instances across 156 annotated images.
  - Train/validation/test split: 70% / 15% / 15%.
    - Training: 109 images (349 ships)
    - Validation: 24 images (73 ships)
    - Test: 23 images (82 ships)


Why this approach?


- Unlike the CNN dataset (cropped 49√ó49 patches), YOLO requires full images with bounding boxes, enabling it to learn both localisation and classification in one pass.
- Using sub-tiles ensured that ships remained visible without overwhelming GPU memory.
- A single-class setup simplified the pipeline while directly aligning with the research goal: detecting ships in Sentinel-2 imagery.


üß† CNN-Based Methodology


The CNN approach was designed to detect ships in Sentinel-2 imagery using transfer learning from state-of-the-art (SOTA) convolutional architectures. The workflow can be summarised as follows:

üîπ Models Used
Eight CNN models were evaluated:
- VGG-16 ‚Äì baseline, simple 3√ó3 conv filters, frozen feature extractor + new classifier.
- ResNet-50 ‚Äì residual connections, added dropout, tuned dense layers.
- Inception-V3 ‚Äì parallel filters, batch norm, increased training epochs.
- NASNetMobile ‚Äì neural architecture search (lightweight, efficient).
- EfficientNetB0 ‚Äì compound scaling for balanced efficiency.
- DenseNet-121 ‚Äì dense connectivity for gradient flow.
- MobileNet-V2 ‚Äì lightweight, designed for real-time/mobile use.
- SimpleNet2021 ‚Äì custom-built CNN trained from scratch as a baseline.


üîπ Experimental Setup
- Environment: Python + TensorFlow 2 on GPU (NVIDIA GTX 1660 Ti).
- Dataset: 1,385 labelled images (5 classes: Ship, Cloud, Land, Sea, Coast).
  - Train/test split = 1067 / 318 images.

- Preprocessing:
  - Pixel values normalised (0‚Äì1).
  - Labels one-hot encoded.
  - Resized to match input requirements (e.g., 224√ó224 for transfer learning).


üîπ Training & Compilation
- Transfer Learning: Pre-trained ImageNet weights with frozen base layers; new classifier layers added.
- Loss Function: Categorical Cross-Entropy.
- Optimiser: Adam / SGD with tuned learning rates.
- Evaluation Metrics: Precision, Recall, F1-score, Accuracy, Confusion Matrix, Classification Report.


üîπ Detection on Large Images
- Used sliding window (49√ó49 crops) to scan 2000√ó2000 scene images.
- Predictions combined into detection maps.
- Applied Non-Maximum Suppression (NMS) to remove duplicate bounding boxes.


üîπ Hyperparameter Optimisation (HPO)
- Random Search used instead of grid search (less computationally heavy).
- Adjustments included neuron counts, dropout rates, and epochs per model.


üîπ Data Augmentation
- Initially tested (rotation, flipping, scaling), but degraded model performance.
- Omitted from final methodology to keep data realistic.


üöÄ YOLOv11-Based Methodology

To overcome the limitations of CNNs (sliding windows, slow inference, limited localisation), a YOLOv11 Nano model was implemented for real-time ship detection in Sentinel-2 imagery. Unlike CNN pipelines, YOLO directly predicts bounding boxes and class labels in a single forward pass, making it efficient for maritime surveillance tasks.


üîπ Why YOLOv11?
- Handles small objects like ships effectively using spatial pyramid pooling and attention mechanisms.
- Lightweight and deployable on resource-constrained devices (e.g., drones, CubeSats).
- Supports real-time detection with high accuracy.


üîπ Experimental Setup
- Framework: Ultralytics YOLOv11, built on PyTorch.
- Dataset: Custom Sentinel-2 dataset (156 annotated images, 504 ship instances).
- Input Size: 640√ó640 pixels (balanced resolution vs. memory).
- Annotation Format: YOLO text files ‚Üí <class> <x_center> <y_center> <width> <height> (normalised).
- Hardware: Trained on Google Colab (Tesla T4 GPU) for speed and scalability.


üîπ Training Configuration
- Epochs: 55 (avoided under/overfitting).
- Batch Size: 16.
- Optimiser: AdamW (auto-configured by Ultralytics).
- Learning Rate: 0.002 with momentum 0.9.
-  Data Augmentation: Mosaic, flips, rotations, scaling applied to improve generalisation.


üîπ Key Loss Functions Monitored
- Box Loss ‚Üí error in bounding box localisation.
- Cls Loss ‚Üí classification accuracy for ‚Äúship‚Äù.
- Dfl Loss ‚Üí confidence/stability in box predictions.


üîπ Evaluation Metrics
- Precision, Recall, F1-Score ‚Üí classification quality.
- IoU & mIoU ‚Üí bounding box overlap accuracy.
- AP & mAP50 / mAP[50‚Äì95] ‚Üí average precision at multiple thresholds.
- ROC & Precision-Recall curves ‚Üí assessed class imbalance and trade-offs.


üîπ Challenges & Fine-Tuning
- No True Negatives: Initially, test data only had ships. TN images were later added to allow full confusion matrix evaluation.
- Missed Detections: Model sometimes failed on small/occluded ships ‚Üí mitigated with augmentation & IoU threshold tuning.
- Hyperparameter Adjustments: Tweaked learning rate, batch size, weight decay for better convergence.
- Early Stopping: Stopped training if validation performance plateaued.


üîπ Outcome
YOLOv11 successfully demonstrated fast and accurate ship detection with potential for real-time maritime monitoring. Despite dataset limitations, the model outperformed CNN approaches in efficiency and scalability, marking it as a practical framework for Maritime Domain Awareness (MDA) applications.


üìä Results & Discussion ‚Äì CNN-Based Approach


üîπ Model Performance
Eight CNN architectures were tested on the custom Sentinel-2 dataset (5 classes: Ship, Cloud, Sea, Land, Coast).


- Top Performers:
  - ResNet-50 ‚Üí Highest validation accuracy (97.8%) but signs of overfitting.
  - VGG-16 & Inception-V3 ‚Üí Strong validation accuracy (95.6%) with more stable convergence.
  - EfficientNetB0 ‚Üí Balanced accuracy (97.1%) and efficiency.
  - MobileNet-V2 ‚Üí Competitive (93.1%) with excellent computational efficiency.


- Weaker Models:
  - DenseNet-121 ‚Üí Oscillations, less stable.
  - NASNetMobile ‚Üí Poor generalisation (82.1% validation accuracy).
  - SimpleNet2021 ‚Üí Lightweight but limited performance (<80% accuracy).


üîπ Confusion Matrix Insights
- ResNet-50 ‚Üí Strongest overall, high true positives (TP) and true negatives (TN).
- VGG-16 ‚Üí Best at detecting ships (only 4 false predictions in Ship class).
- Common Misclassifications:
  - Cloud vs. Sea ‚Äì frequent confusion due to spectral similarity.
  - Land vs. Coast ‚Äì overlaps caused misclassifications.
- SimpleNet2021 ‚Üí Failed to detect ships reliably, mostly misclassifying clouds/land.


üîπ Large Scene Evaluation (2000√ó2000 px images)
- MobileNet-V2 ‚Üí Best generalisation, correctly detected most ships with minimal false positives, though it missed near-shore vessels.
- NASNetMobile & EfficientNetB0 ‚Üí Detected many ships but high false positives.
- ResNet-50, VGG-16, Inception-V3 ‚Üí Despite strong accuracy metrics, showed poor scene-level performance with excessive false positives.
- Key Takeaway: Accuracy on small crops ‚â† robust detection on full-scene satellite images.

<img width="534" height="777" alt="image" src="https://github.com/user-attachments/assets/2ad5423a-278e-4bb6-8e47-bb756477aa57" />


üîπ Optimisation (HPO + NMS)
- Hyperparameter Optimisation (HPO): Improved convergence but varied by model.
- Non-Maximum Suppression (NMS): Reduced redundant bounding boxes and false positives.
- Best F1-Scores after optimisation:
  - Inception-V3 ‚Üí 61.5%
  - ResNet-50 ‚Üí 58.3%
  - EfficientNetB0 ‚Üí 61.1%
- Trade-offs: Higher accuracy models required long training/inference times (e.g., ResNet-50 took ~4.5h for one scene image).

<img width="565" height="804" alt="image" src="https://github.com/user-attachments/assets/e51dff63-1c94-4e99-91f9-fb94204e5565" />

<img width="674" height="302" alt="image" src="https://github.com/user-attachments/assets/5618f391-22b3-43c7-9074-7511e275974a" />


üîπ Testing with MASATI-V2 Dataset
- On MASATI-V2 (larger dataset) ‚Üí All models improved, with ResNet-50 achieving perfect scores.
- Cross-dataset testing (trained on custom dataset, tested on MASATI-V2) ‚Üí Performance dropped significantly, showing that dataset size and diversity are critical.
- Ship Class Focus ‚Üí High precision across most models, confirming robustness for detecting ships even with dataset constraints.
  

‚úÖ Overall Takeaway:
- CNNs can achieve high classification accuracy on small cropped patches, but generalisation to full-scene images remains challenging.
- MobileNet-V2 was the most practical candidate for deployment due to its balance of accuracy, efficiency, and generalisation.
- Larger, more diverse datasets (like MASATI-V2) significantly improve CNN performance, underscoring the importance of dataset quality over model complexity.


üìä Results & Discussion ‚Äì YOLOv11-Based Approach


üîπ Overall Performance
YOLOv11 was evaluated for ship detection in Sentinel-2 imagery, focusing on precision, recall, F1-score, and mean Average Precision (mAP). Both a default YOLOv11 model and a fine-tuned version were tested.
- Default YOLOv11: Achieved respectable results with mAP = 78.05%, but struggled with precise localisation at higher IoU thresholds.
- Fine-Tuned YOLOv11: Outperformed the default, reaching mAP = 85.51%, showing better adaptability and generalisation.


üîπ Training Insights
- Loss curves (Box, Class, DFL) showed consistent decreases, indicating stable learning.
- mAP50 improved steadily to ~98% during training, while mAP50‚Äì95 peaked at ~23%, reflecting challenges in very strict localisation.
- Precision rose rapidly after epoch 6, peaking around 76%, while recall improved more gradually to ~64%. Fine-tuning helped balance both metrics.


üîπ Test Results
- Best operating threshold: IoU = 0.5, confidence > 0.3.
- Confusion Matrix (Fine-Tuned model):
  - True Positives (TP): 53
  - False Positives (FP): 10
  - False Negatives (FN): 6
  - True Negatives (TN): 1
- Achieved high recall (88.1%), critical for maritime surveillance where missing ships is more costly than over-detecting.

<img width="529" height="760" alt="image" src="https://github.com/user-attachments/assets/d4dfe33d-ff39-4ef1-b516-81c0229e550e" />

<img width="515" height="351" alt="image" src="https://github.com/user-attachments/assets/0d3e046a-27a8-4699-9aa2-4caca8e9da1e" />

<img width="510" height="694" alt="image" src="https://github.com/user-attachments/assets/31d36f11-a774-4733-b5c3-ccd7cf3943b2" />

<img width="635" height="624" alt="image" src="https://github.com/user-attachments/assets/c25b7da8-80e1-44ec-9cbc-051f084860c7" />


üîπ Strengths
- Robust against false detections over land, coasts, reefs, and in cloudy/foggy conditions.
- Correctly detected small ships with visible wakes and partial ships cut by image boundaries.
- Demonstrated practical real-time detection potential for maritime domain awareness.


üîπ Limitations
- Missed small ships unless visually distinct (e.g., red vessels).
- Struggled with closely spaced ships and harbour/shoreline clutter.
- Occasionally misclassified cloud patterns as ships.
- Multiple bounding boxes sometimes drawn around the same ship (NMS inefficiencies).
- Challenges due to dataset constraints: limited size, low resolution, and lack of true negatives in early experiments.


üîπ Key Takeaways
- YOLOv11 generalised better than CNNs for full-scene images, offering strong recall and adaptability.
- Fine-tuning significantly improved performance across IoU thresholds, stabilising precision and recall.
- Despite limitations with small objects and dataset size, YOLOv11 showed clear promise as a practical, real-time framework for ship detection in satellite imagery.


‚úÖ Summary: Fine-Tuned YOLOv11 achieved 85.5% mAP, strong recall, and robustness in complex maritime scenes, outperforming traditional CNNs in both efficiency and adaptability.


‚öñÔ∏è Comparative Analysis ‚Äì CNN vs YOLOv11
üîπ Architectural Differences
- CNN Approach:
  - Worked on 49√ó49 image chips, classifying them into Ship, Sea, Land, Coast, or Cloud.
  - Required multiple steps ‚Üí classification ‚Üí merging ‚Üí bounding box placement.
  - Accurate on small chips, but weak when generalising to large full-scene images.


- YOLOv11 Approach:
  - Processes entire images in one pass, predicting both class and bounding box simultaneously.
  - Optimised for real-time detection with fewer preprocessing steps.
  - More effective in cluttered maritime environments (clouds, reefs, coasts).


üîπ Dataset Requirements
- CNN: Needed thousands of small, manually cropped patches, with multiple classes.
- YOLOv11: Used 2000√ó2000 sub-tiles annotated with bounding boxes.
- Result: YOLO dataset was more scalable and context-rich.


üîπ Quantitative Performance (Scene-Level)
<img width="833" height="218" alt="image" src="https://github.com/user-attachments/assets/bb33454c-8bac-4393-847c-695950dccbca" />


üîπ Qualitative Observations
- CNN Limitations:
  - Overfitting (ResNet-50).
  - Frequent false positives in scene images.
  - Missed near-shore and harbour ships due to background clutter.
  - Confusion between Cloud vs Sea, Land vs Coast.

- YOLOv11 Strengths:
  - Fewer false detections in land/coastal/cloudy regions.
  - Better detection of small ships and partially visible ships.
  - More robust in cluttered, real-world maritime scenes.
  - Still struggled with:
    - Misclassifying certain cloud patterns as ships.
    - Detecting closely spaced vessels.
    - Occasional duplicate bounding boxes.


üîπ Efficiency & Scalability
- CNN: Heavy preprocessing (tiling), slow inference, higher memory needs.
- YOLOv11: Entire image processed at once, faster inference and fewer epochs to converge.


üîπ Application Suitability
- CNN: Useful in controlled environments with well-defined classes (e.g., forensic analysis, classification tasks).
- YOLOv11: Better suited for real-world maritime surveillance and real-time applications (live monitoring, drones, CubeSats).
  

‚úÖ Summary:
CNNs achieved high accuracy on small patches but struggled in full-scene detection. YOLOv11 outperformed CNNs across accuracy, recall, and inference speed, making it the more practical and scalable solution for ship detection in Sentinel-2 imagery.

| Feature                 | CNN Approach                               | YOLOv11 Approach                                   |
| ----------------------- | ------------------------------------------ | -------------------------------------------------- |
| **Input Format**        | 49√ó49 patches, multi-class labels          | 2000√ó2000 images with bounding boxes               |
| **Architecture**        | Classification ‚Üí localisation (multi-step) | End-to-end detection (single forward pass)         |
| **Performance (Scene)** | Accuracy 17‚Äì30%, Recall ‚â§ 44%              | Accuracy 77%, Recall 84%                           |
| **Strengths**           | Good at fine-grained classification        | Robust in clutter, real-time detection             |
| **Weaknesses**          | Overfitting, many false positives, slow    | Missed small/closely spaced ships, cloud confusion |
| **Inference Speed**     | 10‚Äì16s per chip                            | 0.03s per full image                               |
| **Best Use Case**       | Controlled classification tasks            | Maritime surveillance, real-time monitoring        |
